package vanillapg

import (
	"fmt"

	G "gorgonia.org/gorgonia"
	"sfneuman.com/golearn/agent"
	"sfneuman.com/golearn/agent/nonlinear/continuous/policy"
	env "sfneuman.com/golearn/environment"
	"sfneuman.com/golearn/network"
)

type Config interface {
	CreateAgent(env.Environment, uint64) (agent.Agent, error)
	ValidAgent(agent.Agent) bool
	Validate() error
}

type PolicyType string

const (
	Gaussian    PolicyType = "Gaussian"
	Categorical PolicyType = "Softmax"
)

// func (c *TreePolicyConfig) ValidAgent(a agent.Agent) bool {
// 	switch a.(type) {
// 	case *VPG:
// 		return true
// 	}
// 	return false
// }

// func (c *TreePolicyConfig) CreateAgent(e env.Environment, seed uint64) (agent.Agent, error) {
// 	if c.Policy != Gaussian {
// 		panic(fmt.Sprintf("createAgent: tree policy %v not implemented",
// 			c.Policy))
// 	}

// 	behaviour, err := policy.NewGaussianTreeMLP(e, 1, G.NewGraph(),
// 		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.LeafLayers,
// 		c.LeafBiases, c.LeafActivations, c.InitWFn, seed)
// 	if err != nil {
// 		return nil, fmt.Errorf("createAgent: could not create "+
// 			"behaviour policy: %v", err)
// 	}

// 	p, err := policy.NewGaussianTreeMLP(e, c.EpochLength, G.NewGraph(),
// 		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.LeafLayers,
// 		c.LeafBiases, c.LeafActivations, c.InitWFn, seed)
// 	if err != nil {
// 		return nil, fmt.Errorf("createAgent: could not create policy: %v", err)
// 	}

// 	features := e.ObservationSpec().Shape.Len()

// 	critic, err := network.NewSingleHeadMLP(features, 1, G.NewGraph(),
// 		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn, c.ValueFnActivations)
// 	if err != nil {
// 		return nil, fmt.Errorf("createAgent: could not create critic: %v", err)
// 	}

// 	trainValueFn, err := network.NewSingleHeadMLP(features, c.EpochLength, G.NewGraph(),
// 		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn, c.ValueFnActivations)
// 	if err != nil {
// 		return nil, fmt.Errorf("createAgent: could not create train critic: %v", err)
// 	}

// 	network.Set(behaviour.Network(), p.Network())
// 	network.Set(critic, trainValueFn)
// 	c.policy = p
// 	c.behaviour = behaviour
// 	c.vValueFn = critic
// 	c.vTrainValueFn = trainValueFn

// 	return New(e, c, int64(seed))
// }

// // What should really happen is that config should take in network architectures
// // like []int, []bool, etc. Then, we can call config.CreateAgent() which
// // will create the networks appropriately (e.g. QValueFn has state obs
// // as top features and action obs as bottom features in revTreeMLP)
// type TreePolicyConfig struct {
// 	// Policy neural net
// 	policy            agent.LogPDFer // VPG.trainPolicy
// 	behaviour         agent.NNPolicy // VPG.behaviour
// 	Policy            PolicyType
// 	PolicyLayers      []int
// 	PolicyBiases      []bool
// 	PolicyActivations []*network.Activation

// 	LeafLayers      [][]int
// 	LeafBiases      [][]bool
// 	LeafActivations [][]*network.Activation

// 	// State value function neural net
// 	vValueFn           network.NeuralNet
// 	vTrainValueFn      network.NeuralNet
// 	ValueFnLayers      []int
// 	ValueFnBiases      []bool
// 	ValueFnActivations []*network.Activation

// 	// Weight init function for all neural nets
// 	InitWFn G.InitWFn

// 	PolicySolver G.Solver
// 	VSolver      G.Solver

// 	// Number of gradient steps to take for the value functions per
// 	// epoch
// 	ValueGradSteps int

// 	EpochLength      int
// 	MaxEpisodeLength int

// 	// Generalized Advantage Estimation
// 	Lambda float64
// 	Gamma  float64
// }

// // BatchSize gets the batch size for the policy generated by this config
// func (c *TreePolicyConfig) BatchSize() int {
// 	return c.EpochLength
// }

// // Validate checks a Config to ensure it is a valid configuration
// func (c *TreePolicyConfig) Validate() error {
// 	if c.EpochLength <= 0 {
// 		return fmt.Errorf("cannot have epoch length < 1")
// 	}

// 	return nil
// }

type CategoricalMLPConfig struct {
	// Policy neural net
	policy            agent.LogPDFer // VPG.trainPolicy
	behaviour         agent.NNPolicy // VPG.behaviour
	Policy            PolicyType
	PolicyLayers      []int
	PolicyBiases      []bool
	PolicyActivations []*network.Activation

	// State value function neural net
	vValueFn           network.NeuralNet
	vTrainValueFn      network.NeuralNet
	ValueFnLayers      []int
	ValueFnBiases      []bool
	ValueFnActivations []*network.Activation

	// Weight init function for all neural nets
	InitWFn G.InitWFn

	PolicySolver G.Solver
	VSolver      G.Solver

	// Number of gradient steps to take for the value functions per
	// epoch
	ValueGradSteps int

	EpochLength      int
	MaxEpisodeLength int

	// Generalized Advantage Estimation
	Lambda float64
	Gamma  float64
}

// BatchSize gets the batch size for the policy generated by this config
func (c *CategoricalMLPConfig) BatchSize() int {
	return c.EpochLength
}

// Validate checks a Config to ensure it is a valid configuration
func (c *CategoricalMLPConfig) Validate() error {
	if c.EpochLength <= 0 {
		return fmt.Errorf("cannot have epoch length < 1")
	}

	return nil
}

func (c *CategoricalMLPConfig) ValidAgent(a agent.Agent) bool {
	switch a.(type) {
	case *VPG:
		return true
	}
	return false
}

func (c *CategoricalMLPConfig) CreateAgent(e env.Environment, seed uint64) (agent.Agent, error) {
	if c.Policy != Categorical {
		panic(fmt.Sprintf("createAgent: mlp policy %v not implemented",
			c.Policy))
	}

	behaviour, err := policy.NewCategoricalMLP(e, 1, G.NewGraph(),
		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.InitWFn, seed)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create "+
			"behaviour policy: %v", err)
	}

	p, err := policy.NewCategoricalMLP(e, c.EpochLength, G.NewGraph(),
		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.InitWFn, seed)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create policy: %v", err)
	}

	features := e.ObservationSpec().Shape.Len()

	critic, err := network.NewSingleHeadMLP(features, 1, G.NewGraph(),
		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn, c.ValueFnActivations)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create critic: %v", err)
	}

	trainValueFn, err := network.NewSingleHeadMLP(features, c.EpochLength, G.NewGraph(),
		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn, c.ValueFnActivations)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create train critic: %v", err)
	}

	network.Set(behaviour.Network(), p.Network())
	network.Set(critic, trainValueFn)
	c.policy = p
	c.behaviour = behaviour
	c.vValueFn = critic
	c.vTrainValueFn = trainValueFn

	return New(e, c, int64(seed))
}
