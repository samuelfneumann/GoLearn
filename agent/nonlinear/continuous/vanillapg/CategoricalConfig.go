package vanillapg

import (
	"fmt"
	"reflect"

	G "gorgonia.org/gorgonia"
	"sfneuman.com/golearn/agent"
	"sfneuman.com/golearn/agent/nonlinear/continuous/policy"
	env "sfneuman.com/golearn/environment"
	"sfneuman.com/golearn/initwfn"
	"sfneuman.com/golearn/network"
	"sfneuman.com/golearn/solver"
)

// CategoricalMLPConfigs implements functionality for storing a list
// of CategoricalMLPConfig in a simple way.
type CategoricalMLPConfigs struct {
	Policy            agent.PolicyType
	PolicyLayers      [][]int
	PolicyBiases      [][]bool
	PolicyActivations [][]*network.Activation

	// State value function neural net
	ValueFnLayers      [][]int
	ValueFnBiases      [][]bool
	ValueFnActivations [][]*network.Activation

	// Weight init function for all neural nets
	InitWFn []*initwfn.InitWFn

	PolicySolver []*solver.Solver
	VSolver      []*solver.Solver

	// Number of gradient steps to take for the value functions per
	// epoch
	ValueGradSteps []int
	EpochLength    []int

	// Generalized Advantage Estimation
	Lambda []float64
	Gamma  []float64
}

// NumFields gets the total number of settable fields/hyperparameters
// for the agent configuration
func (c CategoricalMLPConfigs) NumFields() int {
	rValue := reflect.ValueOf(c)
	return rValue.NumField()
}

// Config returns an empty Config that is of the type stored by
// CategoricalMLPConfigs
func (c CategoricalMLPConfigs) Config() agent.Config {
	return CategoricalMLPConfig{}
}

// Len returns the number of configurations stored by the list
func (c CategoricalMLPConfigs) Len() int {
	return len(c.Lambda) * len(c.Gamma) * len(c.ValueGradSteps) *
		len(c.EpochLength) * len(c.InitWFn) * len(c.ValueFnActivations) *
		len(c.ValueFnBiases) * len(c.ValueFnLayers) * len(c.PolicySolver) *
		len(c.VSolver) * len(c.PolicyActivations) * len(c.PolicyBiases) *
		len(c.PolicyLayers)
}

// CategoricalConfig implements a configuration for a categorical policy
// vanilla policy gradient agent.
type CategoricalMLPConfig struct {
	// Policy neural net
	policy            agent.LogPdfOfer // VPG.trainPolicy
	behaviour         agent.NNPolicy   // VPG.behaviour
	Policy            agent.PolicyType
	PolicyLayers      []int
	PolicyBiases      []bool
	PolicyActivations []*network.Activation

	// State value function neural net
	vValueFn           network.NeuralNet
	vTrainValueFn      network.NeuralNet
	ValueFnLayers      []int
	ValueFnBiases      []bool
	ValueFnActivations []*network.Activation

	// Weight init function for all neural nets
	InitWFn *initwfn.InitWFn

	PolicySolver *solver.Solver
	VSolver      *solver.Solver

	// Number of gradient steps to take for the value functions per
	// epoch
	ValueGradSteps int
	EpochLength    int

	// Generalized Advantage Estimation
	Lambda float64
	Gamma  float64
}

// BatchSize gets the batch size for the policy generated by this config
func (c CategoricalMLPConfig) BatchSize() int {
	return c.EpochLength
}

// Validate checks a Config to ensure it is a valid configuration
func (c CategoricalMLPConfig) Validate() error {
	if c.EpochLength <= 0 {
		return fmt.Errorf("cannot have epoch length < 1")
	}

	if c.Policy != agent.Categorical {
		return fmt.Errorf("cannot create %v policy from categorical "+
			"configuration, must be %v", c.Policy, agent.Categorical)
	}

	return nil
}

// ValidAgent returns whether the input agent is valid for this config
func (c CategoricalMLPConfig) ValidAgent(a agent.Agent) bool {
	switch a.(type) {
	case *VPG:
		return true
	}
	return false
}

// CreateAgent creates and returns the agent determine by the configuration
func (c CategoricalMLPConfig) CreateAgent(e env.Environment,
	seed uint64) (agent.Agent, error) {
	if c.Policy != agent.Categorical {
		panic(fmt.Sprintf("createAgent: mlp policy %v not implemented",
			c.Policy))
	}

	behaviour, err := policy.NewCategoricalMLP(e, 1, G.NewGraph(),
		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.InitWFn.InitWFn(), seed)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create "+
			"behaviour policy: %v", err)
	}

	p, err := policy.NewCategoricalMLP(e, c.EpochLength, G.NewGraph(),
		c.PolicyLayers, c.PolicyBiases, c.PolicyActivations, c.InitWFn.InitWFn(), seed)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create policy: %v", err)
	}

	features := e.ObservationSpec().Shape.Len()

	critic, err := network.NewSingleHeadMLP(features, 1, G.NewGraph(),
		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn.InitWFn(), c.ValueFnActivations)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create critic: %v", err)
	}

	trainValueFn, err := network.NewSingleHeadMLP(features, c.EpochLength, G.NewGraph(),
		c.ValueFnLayers, c.ValueFnBiases, c.InitWFn.InitWFn(), c.ValueFnActivations)
	if err != nil {
		return nil, fmt.Errorf("createAgent: could not create train critic: %v", err)
	}

	network.Set(behaviour.Network(), p.Network())
	network.Set(critic, trainValueFn)
	c.policy = p
	c.behaviour = behaviour
	c.vValueFn = critic
	c.vTrainValueFn = trainValueFn

	return New(e, c, int64(seed))
}
