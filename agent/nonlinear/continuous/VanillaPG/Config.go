package vanillapg

import (
	"fmt"
	"os"

	G "gorgonia.org/gorgonia"
	"sfneuman.com/golearn/agent"
	"sfneuman.com/golearn/network"
)

type TreeCriticConfig struct{}

// What should really happen is that config should take in network architectures
// like []int, []bool, etc. Then, we can call config.CreateAgent() which
// will create the networks appropriately (e.g. QCritic has state obs
// as top features and action obs as bottom features in revTreeMLP)
type SeparateCriticConfig struct {
	// Policy neural net
	Policy agent.PolicyLogProber

	// State value function neural net
	VCritic network.NeuralNet
	// VLayers      []int
	// VBiases      []bool
	// VActivations []*network.Activation

	// Action value function neural net
	// First root network -> state
	// Second root network -> action
	QCritic *network.RevTreeMLP
	// QLayers      []int
	// QBiases      []bool
	// QActivations []*network.Activation

	// Weight init function for all neural nets
	InitWFn G.InitWFn

	PolicySolver G.Solver
	QSolver      G.Solver
	VSolver      G.Solver

	// Number of gradient steps to take for the value functions per
	// epoch
	ValueGradSteps int

	MaxEpisodeLength int
}

// BatchSize gets the batch size for the policy generated by this config
func (c SeparateCriticConfig) BatchSize() int {
	return c.Policy.Network().BatchSize()
}

// Validate checks a Config to ensure it is a valid configuration of a
// DeepQ agent.
func (c *SeparateCriticConfig) Validate() error {
	// // Error checking on state value network
	// if len(c.VLayers) != len(c.VBiases) {
	// 	msg := fmt.Sprintf("new: invalid number of biases for state value "+
	// 		"network \n\twant(%v)\n\thave(%v)", len(c.VLayers),
	// 		len(c.VBiases))
	// 	return fmt.Errorf(msg)
	// }

	// if len(c.VLayers) != len(c.VActivations) {
	// 	msg := fmt.Sprintf("new: invalid number of activations for state "+
	// 		"value network \n\twant(%v)\n\thave(%v)", len(c.VLayers),
	// 		len(c.VActivations))
	// 	return fmt.Errorf(msg)
	// }

	// // Error checking on action value network
	// if len(c.QLayers) != len(c.QBiases) {
	// 	msg := fmt.Sprintf("new: invalid number of biases for action value "+
	// 		"network \n\twant(%v)\n\thave(%v)", len(c.QLayers),
	// 		len(c.QBiases))
	// 	return fmt.Errorf(msg)
	// }

	// if len(c.QLayers) != len(c.QActivations) {
	// 	msg := fmt.Sprintf("new: invalid number of activations for action "+
	// 		"value network \n\twant(%v)\n\thave(%v)", len(c.QLayers),
	// 		len(c.QActivations))
	// 	return fmt.Errorf(msg)
	// }

	if len(c.VCritic.Prediction()) != 1 || len(c.QCritic.Prediction()) != 1 {
		return fmt.Errorf("critics should output a single value")
	}
	if batch, epLen := c.BatchSize(), c.MaxEpisodeLength; batch != epLen && batch != 1 {
		fmt.Fprintf(os.Stderr, "new: changing policy batch size to equal "+
			"maximum episode length")

		policy := c.Policy
		newPolicy, err := policy.CloneWithBatch(c.MaxEpisodeLength)
		if err != nil {
			return fmt.Errorf("new: could not adjust policy batch "+
				"size: %v", err)
		}

		c.Policy = newPolicy.(agent.PolicyLogProber)
	}

	if batch, epLen := c.QCritic.BatchSize(), c.MaxEpisodeLength; batch != epLen {
		fmt.Fprintf(os.Stderr, "new: changing action critic batch size to equal "+
			"maximum episode length")

		critic := c.QCritic
		newCritic, err := critic.CloneWithBatch(c.MaxEpisodeLength)
		if err != nil {
			return fmt.Errorf("new: could not adjust action critic batch "+
				"size: %v", err)
		}

		c.QCritic = newCritic.(*network.RevTreeMLP)
	}

	if batch, epLen := c.VCritic.BatchSize(), c.MaxEpisodeLength; batch != epLen {
		fmt.Fprintf(os.Stderr, "new: changing state critic batch size to equal "+
			"maximum episode length")

		critic := c.VCritic
		newCritic, err := critic.CloneWithBatch(c.MaxEpisodeLength)
		if err != nil {
			return fmt.Errorf("new: could not adjust state critic batch "+
				"size: %v", err)
		}

		c.VCritic = newCritic
	}

	return nil
}
