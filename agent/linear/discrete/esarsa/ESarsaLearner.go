package esarsa

import (
	"fmt"
	"os"

	"github.com/samuelfneumann/golearn/agent/linear/discrete/policy"
	"github.com/samuelfneumann/golearn/timestep"
	"gonum.org/v1/gonum/mat"
)

// ESarsaLearner implements the update functionality for the
// online Expected Sarsa algorithm.
type ESarsaLearner struct {
	weights *mat.Dense

	// Store the latest transition
	step     timestep.TimeStep
	action   int
	nextStep timestep.TimeStep

	learningRate float64

	// ϵ of ϵ-greedy target policy
	targetE float64

	policy *policy.EGreedy // The behaviour policy
	target *policy.EGreedy // The target policy

	// indexTileCoding represents whether the environment is using
	// tile coding and returning the non-zero indices as features
	indexTileCoding bool
}

// NewESarsaLearner creates a new ESarsaLearner struct
//
// egreedy is the policy.EGreedy to learn
func NewESarsaLearner(behaviour, target *policy.EGreedy, learningRate,
	targetE float64, indexTileCoding bool) (*ESarsaLearner, error) {
	// Ensure the behaviour and target share the same weights
	bWeights := behaviour.Weights()
	tWeights := target.Weights()
	for key := range bWeights {
		if bWeights[key] != tWeights[key] {
			fmt.Fprintf(os.Stderr, "newESarsaLearner: behaviour and target "+
				"policies do not share weights. Updates using Step() will "+
				"not be reflected in the target policy.")
		}
	}

	step := timestep.TimeStep{}
	nextStep := timestep.TimeStep{}

	learner := &ESarsaLearner{
		weights:         nil,
		step:            step,
		action:          0,
		nextStep:        nextStep,
		learningRate:    learningRate,
		targetE:         targetE,
		policy:          behaviour,
		target:          target,
		indexTileCoding: indexTileCoding,
	}
	weights := behaviour.Weights()
	err := learner.SetWeights(weights)
	return learner, err
}

// ObserveFirst observes and records the first episodic timestep
func (e *ESarsaLearner) ObserveFirst(t timestep.TimeStep) {
	if !t.First() {
		fmt.Fprintf(os.Stderr, "Warning: ObserveFirst() should only be"+
			"called on the first timestep (current timestep = %d)", t.Number)
	}
	e.step = timestep.TimeStep{}
	e.nextStep = t
}

// Observe observes and records any timestep other than the first timestep
func (e *ESarsaLearner) Observe(action mat.Vector, nextStep timestep.TimeStep) {
	if action.Len() != 1 {
		fmt.Fprintf(os.Stderr, "Warning: value-based methods should not "+
			"have multi-dimensional actions (action dim = %d)", action.Len())
	}
	e.step = e.nextStep
	e.action = int(action.AtVec(0))
	e.nextStep = nextStep
}

// TdError calculates the TD error generated by the learner on some
// transition.
func (e *ESarsaLearner) TdError(t timestep.Transition) float64 {
	if t.Action.Len() > 1 || t.NextAction.Len() > 1 {
		panic("actions should be 1-dimensional")
	}
	action := int(t.Action.AtVec(0))
	actionVal := mat.Dot(e.weights.RowView(action), t.State)

	// Find the next action values
	numActions, _ := e.weights.Dims()
	nextActionValues := mat.NewVecDense(numActions, nil)
	nextActionValues.MulVec(e.weights, t.NextState)

	// Calculate the probability of taking each action under the target
	// policy
	probabilities := e.policy.ActionProbabilities(t.NextState)

	// Calculate the expected Q value under the target policy
	expectedQ := mat.Dot(probabilities, nextActionValues)

	// Calculate the TD error
	tdError := t.Reward + t.Discount*expectedQ - actionVal

	return tdError
}

// stepIndex updates the weights of the Agent's Learner and Policy
// assuming that the last seen feature vector was of the form returned
// by environment/wrappers.IndexTileCoding, that is, the feature vector
// records the indices of non-zero components of a tile-coded state
// observation vector.
func (e *ESarsaLearner) stepIndex() {
	numActions, _ := e.weights.Dims()
	actionValues := mat.NewVecDense(numActions, nil)

	for _, i := range e.nextStep.Observation.RawVector().Data {
		actionValues.AddVec(actionValues, e.weights.ColView(int(i)))
	}

	targetProbs := e.target.ActionProbabilities(e.nextStep.Observation)
	expectedVal := mat.Dot(targetProbs, actionValues)

	// Create the update target
	discount := e.nextStep.Discount
	target := e.nextStep.Reward + discount*expectedVal

	// Find current estimate of the taken action
	currentEstimate := 0.0
	for _, i := range e.step.Observation.RawVector().Data {
		currentEstimate += e.weights.At(e.action, int(i))
	}

	scale := e.learningRate * (target - currentEstimate)

	// Upate weights
	for _, i := range e.step.Observation.RawVector().Data {
		w := e.weights.At(e.action, int(i))
		newW := w + scale
		e.weights.Set(e.action, int(i), newW)
	}
}

// Step updates the weights of the Agent's Learner and Policy
func (e *ESarsaLearner) Step() {
	if e.indexTileCoding {
		e.stepIndex()
	} else {
		numActions, _ := e.weights.Dims()

		// Calculate the action values in the next state
		actionValues := mat.NewVecDense(numActions, nil)
		nextState := e.nextStep.Observation
		actionValues.MulVec(e.weights, nextState)

		// Find the target policy's probability of each action
		targetProbs := e.target.ActionProbabilities(nextState)

		// Create the update target
		discount := e.nextStep.Discount
		expectedQ := mat.Dot(targetProbs, actionValues)
		target := e.nextStep.Reward + discount*expectedQ

		// Find the current estimate of the taken action
		weights := e.weights.RowView(e.action)
		state := e.step.Observation
		currentEstimate := mat.Dot(weights, state)

		// Construct the scaling factor of the gradient
		scale := e.learningRate * (target - currentEstimate)

		// Perform gradient descent: ∇weights = scale * state
		weights.(*mat.VecDense).AddScaledVec(weights, scale, state)
	}
}

// SetWeights sets the weight pointers to point to a new set of weights.
// The SetWeights function can take the output of a call to Weights()
// on another Learner or Linear Policy that has a key "weights"
func (e *ESarsaLearner) SetWeights(weights map[string]*mat.Dense) error {
	newWeights, ok := weights[policy.WeightsKey]
	if !ok {
		return fmt.Errorf("SetWeights: no weights named \"%v\"",
			policy.WeightsKey)
	}

	e.weights = newWeights
	return nil
}

// Cleanup at the end of an episode
func (e *ESarsaLearner) EndEpisode() {}
