package examples

import (
	"fmt"
	"time"

	"github.com/samuelfneumann/golearn/agent/linear/discrete/qlearning"
	"github.com/samuelfneumann/golearn/environment"
	"github.com/samuelfneumann/golearn/environment/classiccontrol/mountaincar"
	"github.com/samuelfneumann/golearn/environment/wrappers"
	"github.com/samuelfneumann/golearn/experiment"
	"github.com/samuelfneumann/golearn/experiment/tracker"
	"github.com/samuelfneumann/golearn/utils/matutils/initializers/weights"
	"gonum.org/v1/gonum/spatial/r1"
)

func QlearningMountainCar() {
	var seed uint64 = 1923812

	// Create the QLearning config
	args := qlearning.Config{Epsilon: 0.25, LearningRate: 0.01}

	// Generate the starting state distribution
	positionBounds := r1.Interval{Min: -0.2, Max: 0.2}
	speedBounds := r1.Interval{Min: -0.005, Max: 0.005}
	s := environment.NewUniformStarter([]r1.Interval{positionBounds,
		speedBounds}, seed)

	// Generate the task to complete on the environment
	maxEpisodeSteps := 1000
	goalPosition := 0.45
	task := mountaincar.NewGoal(s, maxEpisodeSteps, goalPosition)

	// Create the Mountain Car environment
	discount := 1.0
	env, _, err := mountaincar.NewDiscrete(task, discount)
	if err != nil {
		panic(err)
	}
	fmt.Println(env)

	// To use Linear Q-learning, we need to tile code the environment
	// states. Tile coding requires that each state observation
	// dimension be bounded. The wrappers.TileCoding struct will take
	// care of bounding the state dimensions for us, we just have to
	// choose the number of tilings and the tiles for each dimension
	// per tiling. Here, we create 5 tilings of size 5x5 tiles and
	// 5 tilings of size 3x3 tiles for our TileCoder.
	numTilings := 10
	tilings := make([][]int, numTilings)
	for i := 0; i < len(tilings)/2; i++ {
		tilings[i] = []int{5, 5}
	}
	for i := len(tilings) / 2; i < len(tilings); i++ {
		tilings[i] = []int{3, 3}
	}
	tm, _, err := wrappers.NewTileCoding(env, tilings, seed)
	if err != nil {
		panic(err)
	}

	// Create the Q-learning algorithm. First, we defined an initialization
	// method for the Linear Q-learning algorithm's weights.
	// First, we need to create an RNG that will sample weights for us.
	rand := weights.NewZeroUV() // Zero RNG

	// Create the weight initializer with the RNG
	init := weights.NewLinearUV(rand)

	// Create the Q-learning algorithm
	q, err := qlearning.New(tm, args, init, seed)
	if err != nil {
		panic(err)
	}

	// Now, we will create the experiment. First, generate savers to
	// determine what data from the experiment we want to save
	saver := tracker.NewReturn("./data.bin")

	// Create a new Online experiment. Online experiments will only
	// run the agent online, and no offline evaluation will occur
	e := experiment.NewOnline(tm, q, 1_00_000, []tracker.Tracker{saver}, nil)

	// Run the experiment
	start := time.Now()
	e.Run()
	fmt.Println("Elapsed:", time.Since(start))

	// Save the data generated by the experiment
	e.Save()

	// Load the data from the experiment and pring it
	data := tracker.LoadFData("./data.bin")
	fmt.Println(data)
}
